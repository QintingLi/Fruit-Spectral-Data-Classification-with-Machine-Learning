# -*- coding: utf-8 -*-
"""水果分类任务.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-n65VTMcb8BxlMIIeg8bZ9QBQOLxYqpk
"""

from google.colab import drive
drive.mount('/content/drive')

import os

# 数据集路径
INTENSITY_DATA_PATH = '/content/drive/My Drive/Intensity_Dataset'
ACTIVE_LIGHT_DATA_PATH = '/content/drive/My Drive/Reflectance_Dataset'

# 读取目录下的所有文件，并打印文件名
def list_files_in_directory(directory):
    filenames = []
    for root, dirs, files in os.walk(directory):
        for file in files:
            if file.endswith('.csv'):
                filenames.append(file)
    return filenames

# 列出 Intensity_Dataset 和 Reflectance_Dataset 中的文件名
intensity_files = list_files_in_directory(INTENSITY_DATA_PATH)
reflectance_files = list_files_in_directory(ACTIVE_LIGHT_DATA_PATH)

print("Intensity Dataset 中的文件名:")
print(intensity_files)

print("\nReflectance Dataset 中的文件名:")
print(reflectance_files)

"""简单的数据合并流程，使用直接拼接的方式。将两个不同目录下的所有CSV文件读取并合并成一个大的数据集。

使用列表存储所有DataFrame，不考虑文件间的对应关系
通过垂直拼接（按行连接）的方式合并所有数据，可能导致数据之间的关联性丢失
"""

import os
import pandas as pd

# 数据集路径
INTENSITY_DATA_PATH = '/content/drive/My Drive/Intensity_Dataset'
ACTIVE_LIGHT_DATA_PATH = '/content/drive/My Drive/Reflectance_Dataset'

# 读取目录下的所有文件，并返回文件名
def list_files_in_directory(directory):
    filenames = []
    for root, dirs, files in os.walk(directory):
        for file in files:
            if file.endswith('.csv'):
                filenames.append(file)
    return filenames

# 读取 CSV 文件并添加标签
def read_csv_with_labels(directory, filenames):
    data_frames = []
    for filename in filenames:
        file_path = os.path.join(directory, filename)
        try:
            # 读取 CSV 文件，尝试使用 UTF-8 编码
            df = pd.read_csv(file_path, encoding='utf-8')
        except UnicodeDecodeError:
            # 如果 UTF-8 解码失败，尝试使用 ISO-8859-1 编码
            df = pd.read_csv(file_path, encoding='ISO-8859-1')
        except Exception as e:
            print(f"无法读取文件 {filename}，错误: {e}")
            continue
        # 为每个 DataFrame 添加标签，标签为文件名
        df['label'] = filename
        data_frames.append(df)
    return data_frames

# 列出 Intensity_Dataset 和 Reflectance_Dataset 中的文件名
intensity_files = list_files_in_directory(INTENSITY_DATA_PATH)
reflectance_files = list_files_in_directory(ACTIVE_LIGHT_DATA_PATH)

# 读取并合并两个数据集的文件
intensity_data = read_csv_with_labels(INTENSITY_DATA_PATH, intensity_files)
reflectance_data = read_csv_with_labels(ACTIVE_LIGHT_DATA_PATH, reflectance_files)

# 将数据合并在一起
merged_data = pd.concat(intensity_data + reflectance_data, ignore_index=True)

# 打印合并后的数据
print("合并后的数据集：")
print(merged_data)

# 可以将合并后的数据保存为一个新的 CSV 文件
merged_data.to_csv('/content/drive/My Drive/merged_dataset.csv', index=False)

"""首先确保只处理在两个数据集中都存在的文件，然后按照文件对应关系进行合并。

使用字典存储DataFrame，保证了数据的可追溯性和对应关系
通过水平拼接（按列连接）的方式合并相同文件的数据，保持了数据的结构完整性
"""

import os
import pandas as pd

# 数据集路径
INTENSITY_DATA_PATH = '/content/drive/My Drive/Intensity_Dataset'
ACTIVE_LIGHT_DATA_PATH = '/content/drive/My Drive/Reflectance_Dataset'

# 读取目录下的所有文件，并返回文件名
def list_files_in_directory(directory):
    filenames = []
    for root, dirs, files in os.walk(directory):
        for file in files:
            if file.endswith('.csv'):
                filenames.append(file)
    return filenames

# 读取 CSV 文件并添加标签
def read_csv_with_labels(directory, filenames):
    data_frames = {}
    for filename in filenames:
        file_path = os.path.join(directory, filename)
        try:
            # 读取 CSV 文件，尝试使用 UTF-8 编码
            df = pd.read_csv(file_path, encoding='utf-8')
        except UnicodeDecodeError:
            # 如果 UTF-8 解码失败，尝试使用 ISO-8859-1 编码
            df = pd.read_csv(file_path, encoding='ISO-8859-1')
        except Exception as e:
            print(f"无法读取文件 {filename}，错误: {e}")
            continue
        # 将 DataFrame 存储在字典中，以文件名作为键
        data_frames[filename] = df
    return data_frames

# 列出 Intensity_Dataset 和 Reflectance_Dataset 中的文件名
intensity_files = list_files_in_directory(INTENSITY_DATA_PATH)
reflectance_files = list_files_in_directory(ACTIVE_LIGHT_DATA_PATH)

# 获取两个数据集中的交集文件名
common_files = list(set(intensity_files) & set(reflectance_files))

# 读取两个数据集的相同文件名数据
intensity_data = read_csv_with_labels(INTENSITY_DATA_PATH, common_files)
reflectance_data = read_csv_with_labels(ACTIVE_LIGHT_DATA_PATH, common_files)

# 合并数据：将反射率和强度数据根据文件名合并为一个 DataFrame
merged_data_frames = []
for file in common_files:
    intensity_df = intensity_data[file]
    reflectance_df = reflectance_data[file]
    # 合并强度和反射率数据，按列拼接
    merged_df = pd.concat([intensity_df, reflectance_df], axis=1)
    # 添加标签列
    merged_df['label'] = file
    merged_data_frames.append(merged_df)

# 将所有合并后的数据拼接成一个大的 DataFrame
final_merged_data = pd.concat(merged_data_frames, ignore_index=True)

# 打印两个数据集的文件名进行比较
print("Intensity Dataset 的文件名:")
for file in intensity_files:
    print(file)

print("\nReflectance Dataset 的文件名:")
for file in reflectance_files:
    print(file)

# 打印合并后的数据集
print("\n合并后的数据集：")
print(final_merged_data)

# 保存合并后的数据为 CSV 文件
final_merged_data.to_csv('/content/drive/My Drive/merged_dataset.csv', index=False)

"""# **SVM**"""

import os
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.svm import SVC
from sklearn.metrics import classification_report

# 数据集路径
MERGED_DATA_PATH = '/content/drive/My Drive/merged_dataset.csv'

# 1. 读取合并后的数据
final_merged_data = pd.read_csv(MERGED_DATA_PATH)

# 2. 检查数据基本信息
print("\n合并后的数据集基本信息：")
print(final_merged_data.info())

# 3. 打印前几行，预览数据
print("\n合并后的数据集预览：")
print(final_merged_data.head())

# 4. 数据清洗：删除没有用的数据列（如"label"列）
# 将标签列移到最后，去掉多余的非数值型列
data = final_merged_data.drop(columns=['label'], axis=1)

# 5. 将数据中的非数值类型转换为数值
# 将无法转换的值变成 NaN
data = data.apply(pd.to_numeric, errors='coerce')

# 6. 处理缺失值
# 删除含有 NaN 的行
data_cleaned = data.dropna()

# 7. 确保标签列 'label' 也同步删除对应的行
y_cleaned = final_merged_data['label'][data_cleaned.index]

# 8. 数据检查：检查是否存在非数值数据
if not np.issubdtype(data_cleaned.values.dtype, np.number):
    print("数据集中仍然存在非数值型数据，请检查并清理数据。")
    exit()

# 9. 特征缩放
# 采用标准化，将每列数据转换为均值为0，标准差为1
scaler = StandardScaler()
X_scaled = scaler.fit_transform(data_cleaned)  # 特征缩放

# 10. 标签编码：将标签列 'label' 转换为数值
le = LabelEncoder()
y_encoded = le.fit_transform(y_cleaned)  # 标签编码

# 11. 划分训练集和测试集：80% 训练集，20% 测试集
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded)

# 12. SVM 分类器训练
svm = SVC(kernel='rbf', C=1.0, gamma='auto')
svm.fit(X_train, y_train)  # 训练模型

# 13. 评估模型
svm_predictions = svm.predict(X_test)  # 预测

# 14. 打印分类报告
print("\nSVM 分类结果：")
print(classification_report(y_test, svm_predictions, target_names=le.classes_))

# 15. 输出测试集准确率
svm_accuracy = svm.score(X_test, y_test)
print(f"SVM 测试集准确率: {svm_accuracy}")

"""# **RandomForest**"""

from sklearn.ensemble import RandomForestClassifier

# 12. 随机森林分类器训练
rf = RandomForestClassifier(n_estimators=100, random_state=42)
rf.fit(X_train, y_train)  # 训练模型

# 13. 评估模型
rf_predictions = rf.predict(X_test)  # 预测

# 14. 打印分类报告
print("\n随机森林分类结果：")
print(classification_report(y_test, rf_predictions, target_names=le.classes_))

# 15. 输出测试集准确率
rf_accuracy = rf.score(X_test, y_test)
print(f"随机森林测试集准确率: {rf_accuracy}")

"""# **KNN**"""

import os
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import classification_report

# 数据集路径
MERGED_DATA_PATH = '/content/drive/My Drive/merged_dataset.csv'

# 1. 读取合并后的数据
final_merged_data = pd.read_csv(MERGED_DATA_PATH)

# 2. 检查数据基本信息
print("\n合并后的数据集基本信息：")
print(final_merged_data.info())

# 3. 打印前几行，预览数据
print("\n合并后的数据集预览：")
print(final_merged_data.head())

# 4. 数据清洗：删除没有用的数据列（如"label"列）
# 将标签列移到最后，去掉多余的非数值型列
data = final_merged_data.drop(columns=['label'], axis=1)

# 5. 将数据中的非数值类型转换为数值
# 将无法转换的值变成 NaN
data = data.apply(pd.to_numeric, errors='coerce')

# 6. 处理缺失值
# 删除含有 NaN 的行
data_cleaned = data.dropna()

# 7. 确保标签列 'label' 也同步删除对应的行
y_cleaned = final_merged_data['label'][data_cleaned.index]

# 8. 数据检查：检查是否存在非数值数据
if not np.issubdtype(data_cleaned.values.dtype, np.number):
    print("数据集中仍然存在非数值型数据，请检查并清理数据。")
    exit()

# 9. 特征缩放
# 采用标准化，将每列数据转换为均值为0，标准差为1
scaler = StandardScaler()
X_scaled = scaler.fit_transform(data_cleaned)  # 特征缩放

# 10. 标签编码：将标签列 'label' 转换为数值
le = LabelEncoder()
y_encoded = le.fit_transform(y_cleaned)  # 标签编码

# 11. 划分训练集和测试集：80% 训练集，20% 测试集
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded)

# 12. KNN 分类器训练
knn = KNeighborsClassifier(n_neighbors=5)  # 使用 5 个邻居
knn.fit(X_train, y_train)  # 训练模型

# 13. 评估模型
knn_predictions = knn.predict(X_test)  # 预测

# 14. 打印分类报告
print("\nKNN 分类结果：")
print(classification_report(y_test, knn_predictions, target_names=le.classes_))

# 15. 输出测试集准确率
knn_accuracy = knn.score(X_test, y_test)
print(f"KNN 测试集准确率: {knn_accuracy}")

"""# **原始的CNN构架**"""

import os
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.metrics import classification_report
import tensorflow as tf
from tensorflow.keras import layers, models

# 数据集路径
MERGED_DATA_PATH = '/content/drive/My Drive/merged_dataset.csv'

# 1. 读取合并后的数据
final_merged_data = pd.read_csv(MERGED_DATA_PATH)

# 2. 检查数据基本信息
print("\n合并后的数据集基本信息：")
print(final_merged_data.info())

# 3. 打印前几行，预览数据
print("\n合并后的数据集预览：")
print(final_merged_data.head())

# 4. 数据清洗：删除没有用的数据列（如"label"列）
# 将标签列移到最后，去掉多余的非数值型列
data = final_merged_data.drop(columns=['label'], axis=1)

# 5. 将数据中的非数值类型转换为数值
# 将无法转换的值变成 NaN
data = data.apply(pd.to_numeric, errors='coerce')

# 6. 处理缺失值
# 删除含有 NaN 的行
data_cleaned = data.dropna()

# 7. 确保标签列 'label' 也同步删除对应的行
y_cleaned = final_merged_data['label'][data_cleaned.index]

# 8. 数据检查：检查是否存在非数值数据
if not np.issubdtype(data_cleaned.values.dtype, np.number):
    print("数据集中仍然存在非数值型数据，请检查并清理数据。")
    exit()

# 9. 特征缩放
# 采用标准化，将每列数据转换为均值为0，标准差为1
scaler = StandardScaler()
X_scaled = scaler.fit_transform(data_cleaned)  # 特征缩放

# 10. 标签编码：将标签列 'label' 转换为数值
le = LabelEncoder()
y_encoded = le.fit_transform(y_cleaned)  # 标签编码

# 11. 划分训练集和测试集：80% 训练集，20% 测试集
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded)

# 12. 调整输入数据形状：CNN 要求输入是三维数据
# 1D CNN 的输入形状通常是 (samples, time_steps, features)
# 假设每个样本的特征数是 `X_train.shape[1]`，将其转化为符合 CNN 输入要求的形状
X_train_reshaped = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)  # 将输入转化为 (batch_size, time_steps, features)
X_test_reshaped = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)  # 将测试集输入转化为相同的形状

# 13. 构建 1D CNN 模型
model = models.Sequential()

# 添加第一层卷积层，32 个滤波器，卷积核大小为 3，激活函数使用 ReLU
model.add(layers.Conv1D(32, 3, activation='relu', input_shape=(X_train_reshaped.shape[1], 1)))

# 添加池化层（最大池化），池化大小为 2
model.add(layers.MaxPooling1D(2))

# 添加第二层卷积层，64 个滤波器，卷积核大小为 3，激活函数使用 ReLU
model.add(layers.Conv1D(64, 3, activation='relu'))

# 添加池化层（最大池化），池化大小为 2
model.add(layers.MaxPooling1D(2))

# 添加展平层，将多维输入一维化
model.add(layers.Flatten())

# 添加全连接层，128 个神经元，激活函数使用 ReLU
model.add(layers.Dense(128, activation='relu'))

# 添加输出层，输出类别的数量
model.add(layers.Dense(len(np.unique(y_encoded)), activation='softmax'))

# 14. 编译模型
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

# 15. 训练模型
model.fit(X_train_reshaped, y_train, epochs=10, batch_size=32, validation_data=(X_test_reshaped, y_test))

# 16. 评估模型
loss, accuracy = model.evaluate(X_test_reshaped, y_test)
print(f"\nCNN 测试集准确率: {accuracy}")

# 17. 预测并打印分类报告
y_pred = model.predict(X_test_reshaped)
y_pred_classes = np.argmax(y_pred, axis=1)

print("\nCNN 分类结果：")
print(classification_report(y_test, y_pred_classes, target_names=le.classes_))

"""# **1D RESNET**"""

import os
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.metrics import classification_report
import tensorflow as tf
from tensorflow.keras import layers, models

# 数据集路径
MERGED_DATA_PATH = '/content/drive/My Drive/merged_dataset.csv'

# 1. 读取合并后的数据
final_merged_data = pd.read_csv(MERGED_DATA_PATH)

# 2. 检查数据基本信息
print("\n合并后的数据集基本信息：")
print(final_merged_data.info())

# 3. 打印前几行，预览数据
print("\n合并后的数据集预览：")
print(final_merged_data.head())

# 4. 数据清洗：删除没有用的数据列（如"label"列）
# 将标签列移到最后，去掉多余的非数值型列
data = final_merged_data.drop(columns=['label'], axis=1)

# 5. 将数据中的非数值类型转换为数值
# 将无法转换的值变成 NaN
data = data.apply(pd.to_numeric, errors='coerce')

# 6. 处理缺失值
# 删除含有 NaN 的行
data_cleaned = data.dropna()

# 7. 确保标签列 'label' 也同步删除对应的行
y_cleaned = final_merged_data['label'][data_cleaned.index]

# 8. 数据检查：检查是否存在非数值数据
if not np.issubdtype(data_cleaned.values.dtype, np.number):
    print("数据集中仍然存在非数值型数据，请检查并清理数据。")
    exit()

# 9. 特征缩放
# 采用标准化，将每列数据转换为均值为0，标准差为1
scaler = StandardScaler()
X_scaled = scaler.fit_transform(data_cleaned)  # 特征缩放

# 10. 标签编码：将标签列 'label' 转换为数值
le = LabelEncoder()
y_encoded = le.fit_transform(y_cleaned)  # 标签编码

# 11. 划分训练集和测试集：80% 训练集，20% 测试集
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded)

# 12. 调整输入数据形状：CNN 要求输入是三维数据
# 1D CNN 的输入形状通常是 (samples, time_steps, features)
# 假设每个样本的特征数是 `X_train.shape[1]`，将其转化为符合 CNN 输入要求的形状
X_train_reshaped = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)  # 将输入转化为 (batch_size, time_steps, features)
X_test_reshaped = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)  # 将测试集输入转化为相同的形状

# 13. 构建 1D ResNet 模型

def residual_block(x, filters, kernel_size=3, stride=1):
    """
    定义一个残差块，其中包含卷积层、批标准化和ReLU激活
    """
    shortcut = x  # 保存输入，用于后面的跳跃连接

    # 第一层卷积
    x = layers.Conv1D(filters, kernel_size=kernel_size, strides=stride, padding='same')(x)
    x = layers.BatchNormalization()(x)
    x = layers.ReLU()(x)

    # 第二层卷积
    x = layers.Conv1D(filters, kernel_size=kernel_size, strides=stride, padding='same')(x)
    x = layers.BatchNormalization()(x)

    # 如果输入与输出的形状不同，我们需要使用 1x1 卷积调整输入的形状
    if shortcut.shape[-1] != x.shape[-1]:
        shortcut = layers.Conv1D(filters, kernel_size=1, strides=stride, padding='same')(shortcut)

    # 跳跃连接：将输入加到输出上
    x = layers.Add()([x, shortcut])
    x = layers.ReLU()(x)

    return x

# 13.1. 输入层
input_layer = layers.Input(shape=(X_train_reshaped.shape[1], 1))

# 13.2. 添加几个残差块
x = residual_block(input_layer, filters=32)
x = residual_block(x, filters=64)
x = residual_block(x, filters=128)

# 13.3. 展平层
x = layers.Flatten()(x)

# 13.4. 全连接层
x = layers.Dense(128, activation='relu')(x)

# 13.5. 输出层，类别数根据标签数设置
output_layer = layers.Dense(len(np.unique(y_encoded)), activation='softmax')(x)

# 13.6. 创建模型
model = models.Model(inputs=input_layer, outputs=output_layer)

# 14. 编译模型
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

# 15. 训练模型
model.fit(X_train_reshaped, y_train, epochs=10, batch_size=32, validation_data=(X_test_reshaped, y_test))

# 16. 评估模型
loss, accuracy = model.evaluate(X_test_reshaped, y_test)
print(f"\nResNet 测试集准确率: {accuracy}")

# 17. 预测并打印分类报告
y_pred = model.predict(X_test_reshaped)
y_pred_classes = np.argmax(y_pred, axis=1)

print("\nResNet 分类结果：")
print(classification_report(y_test, y_pred_classes, target_names=le.classes_))

"""# **RESNET结构图**"""

from IPython.display import Image

# 显示结构图
Image(filename='resnet_model.png')

"""# **双通道 注意力机制**"""

import os
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.metrics import classification_report
import tensorflow as tf
from tensorflow.keras import layers, models

# 数据集路径
INTENSITY_DATA_PATH = '/content/drive/My Drive/Intensity_Dataset'
ACTIVE_LIGHT_DATA_PATH = '/content/drive/My Drive/Reflectance_Dataset'
MERGED_DATA_PATH = '/content/drive/My Drive/merged_dataset.csv'

# 1. 读取合并后的数据
final_merged_data = pd.read_csv(MERGED_DATA_PATH)

# 2. 检查数据基本信息
print("\n合并后的数据集基本信息：")
print(final_merged_data.info())

# 3. 打印前几行，预览数据
print("\n合并后的数据集预览：")
print(final_merged_data.head())

# 4. 数据清洗：删除没有用的数据列（如"label"列）
# 将标签列移到最后，去掉多余的非数值型列
data = final_merged_data.drop(columns=['label'], axis=1)

# 5. 将数据中的非数值类型转换为数值
# 将无法转换的值变成 NaN
data = data.apply(pd.to_numeric, errors='coerce')

# 6. 处理缺失值
# 删除含有 NaN 的行
data_cleaned = data.dropna()

# 7. 确保标签列 'label' 也同步删除对应的行
y_cleaned = final_merged_data['label'][data_cleaned.index]

# 8. 数据检查：检查是否存在非数值数据
if not np.issubdtype(data_cleaned.values.dtype, np.number):
    print("数据集中仍然存在非数值型数据，请检查并清理数据。")
    exit()

# 9. 特征缩放
# 采用标准化，将每列数据转换为均值为0，标准差为1
scaler = StandardScaler()
X_scaled = scaler.fit_transform(data_cleaned)  # 特征缩放

# 10. 标签编码：将标签列 'label' 转换为数值
le = LabelEncoder()
y_encoded = le.fit_transform(y_cleaned)  # 标签编码

# 11. 划分训练集和测试集：80% 训练集，20% 测试集
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded)

# 12. 调整输入数据形状：CNN 要求输入是三维数据
# 1D CNN 的输入形状通常是 (samples, time_steps, features)
X_train_reshaped = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)  # 将输入转化为 (batch_size, time_steps, features)
X_test_reshaped = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)  # 将测试集输入转化为相同的形状

# 自定义注意力层
class AttentionLayer(layers.Layer):
    def __init__(self, **kwargs):
        super(AttentionLayer, self).__init__(**kwargs)

    def build(self, input_shape):
        # 定义注意力权重矩阵
        self.attention_weights = self.add_weight(name='attention_weights',
                                                 shape=(input_shape[-1], 1),
                                                 initializer='random_normal',
                                                 trainable=True)
        super(AttentionLayer, self).build(input_shape)

    def call(self, inputs):
        # 计算注意力权重
        attention_score = tf.matmul(inputs, self.attention_weights)
        attention_score = tf.nn.softmax(attention_score, axis=1)
        # 按照权重加权平均
        weighted_output = inputs * attention_score
        return tf.reduce_sum(weighted_output, axis=1)

# 13. 构建 1D 双通道 CNN 模型

# 添加双输入通道的卷积层
input_intensity = layers.Input(shape=(X_train_reshaped.shape[1], 1))
input_reflectance = layers.Input(shape=(X_train_reshaped.shape[1], 1))

# 第一个通道：Intensity 数据卷积层
x_intensity = layers.Conv1D(32, 3, activation='relu', padding='same')(input_intensity)
x_intensity = layers.MaxPooling1D(2)(x_intensity)
x_intensity = layers.Conv1D(64, 3, activation='relu', padding='same')(x_intensity)
x_intensity = layers.MaxPooling1D(2)(x_intensity)

# 第二个通道：Reflectance 数据卷积层
x_reflectance = layers.Conv1D(32, 3, activation='relu', padding='same')(input_reflectance)
x_reflectance = layers.MaxPooling1D(2)(x_reflectance)
x_reflectance = layers.Conv1D(64, 3, activation='relu', padding='same')(x_reflectance)
x_reflectance = layers.MaxPooling1D(2)(x_reflectance)

# 合并两个通道的特征
x = layers.Concatenate()([x_intensity, x_reflectance])

# 应用注意力层
x = AttentionLayer()(x)

# 展平层，将多维输入一维化
x = layers.Flatten()(x)

# 添加全连接层，128 个神经元，激活函数使用 ReLU
x = layers.Dense(128, activation='relu')(x)

# 添加输出层，输出类别的数量
output_layer = layers.Dense(len(np.unique(y_encoded)), activation='softmax')(x)

# 14. 编译模型
model = models.Model(inputs=[input_intensity, input_reflectance], outputs=output_layer)
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

# 15. 训练模型
model.fit([X_train_reshaped, X_train_reshaped], y_train, epochs=10, batch_size=32, validation_data=([X_test_reshaped, X_test_reshaped], y_test))

# 16. 评估模型
loss, accuracy = model.evaluate([X_test_reshaped, X_test_reshaped], y_test)
print(f"\n双通道 CNN + 注意力机制 测试集准确率: {accuracy}")

# 17. 预测并打印分类报告
y_pred = model.predict([X_test_reshaped, X_test_reshaped])
y_pred_classes = np.argmax(y_pred, axis=1)

print("\n双通道 CNN + 注意力机制 分类结果：")
print(classification_report(y_test, y_pred_classes, target_names=le.classes_))

"""# **1d cnn** + 注意力机制"""

import os
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.metrics import classification_report
import tensorflow as tf
from tensorflow.keras import layers, models

# 数据集路径
INTENSITY_DATA_PATH = '/content/drive/My Drive/Intensity_Dataset'
ACTIVE_LIGHT_DATA_PATH = '/content/drive/My Drive/Reflectance_Dataset'
MERGED_DATA_PATH = '/content/drive/My Drive/merged_dataset.csv'

# 1. 读取合并后的数据
final_merged_data = pd.read_csv(MERGED_DATA_PATH)

# 2. 检查数据基本信息
print("\n合并后的数据集基本信息：")
print(final_merged_data.info())

# 3. 打印前几行，预览数据
print("\n合并后的数据集预览：")
print(final_merged_data.head())

# 4. 数据清洗：删除没有用的数据列（如"label"列）
# 将标签列移到最后，去掉多余的非数值型列
data = final_merged_data.drop(columns=['label'], axis=1)

# 5. 将数据中的非数值类型转换为数值
# 将无法转换的值变成 NaN
data = data.apply(pd.to_numeric, errors='coerce')

# 6. 处理缺失值
# 删除含有 NaN 的行
data_cleaned = data.dropna()

# 7. 确保标签列 'label' 也同步删除对应的行
y_cleaned = final_merged_data['label'][data_cleaned.index]

# 8. 数据检查：检查是否存在非数值数据
if not np.issubdtype(data_cleaned.values.dtype, np.number):
    print("数据集中仍然存在非数值型数据，请检查并清理数据。")
    exit()

# 9. 特征缩放
# 采用标准化，将每列数据转换为均值为0，标准差为1
scaler = StandardScaler()
X_scaled = scaler.fit_transform(data_cleaned)  # 特征缩放

# 10. 标签编码：将标签列 'label' 转换为数值
le = LabelEncoder()
y_encoded = le.fit_transform(y_cleaned)  # 标签编码

# 11. 划分训练集和测试集：80% 训练集，20% 测试集
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded)

# 12. 调整输入数据形状：CNN 要求输入是三维数据
# 1D CNN 的输入形状通常是 (samples, time_steps, features)
X_train_reshaped = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)  # 将输入转化为 (batch_size, time_steps, features)
X_test_reshaped = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)  # 将测试集输入转化为相同的形状

# 自定义注意力层
class AttentionLayer(layers.Layer):
    def __init__(self, **kwargs):
        super(AttentionLayer, self).__init__(**kwargs)

    def build(self, input_shape):
        # 定义注意力权重矩阵
        self.attention_weights = self.add_weight(name='attention_weights',
                                                 shape=(input_shape[-1], 1),
                                                 initializer='random_normal',
                                                 trainable=True)
        super(AttentionLayer, self).build(input_shape)

    def call(self, inputs):
        # 计算注意力权重
        attention_score = tf.matmul(inputs, self.attention_weights)
        attention_score = tf.nn.softmax(attention_score, axis=1)
        # 按照权重加权平均
        weighted_output = inputs * attention_score
        return tf.reduce_sum(weighted_output, axis=1)

# 13. 构建 1D 双通道 CNN 模型

# 添加双输入通道的卷积层
input_intensity = layers.Input(shape=(X_train_reshaped.shape[1], 1))
input_reflectance = layers.Input(shape=(X_train_reshaped.shape[1], 1))

# 第一个通道：Intensity 数据卷积层
x_intensity = layers.Conv1D(32, 3, activation='relu', padding='same')(input_intensity)
x_intensity = layers.MaxPooling1D(2)(x_intensity)
x_intensity = layers.Conv1D(64, 3, activation='relu', padding='same')(x_intensity)
x_intensity = layers.MaxPooling1D(2)(x_intensity)

# 第二个通道：Reflectance 数据卷积层
x_reflectance = layers.Conv1D(32, 3, activation='relu', padding='same')(input_reflectance)
x_reflectance = layers.MaxPooling1D(2)(x_reflectance)
x_reflectance = layers.Conv1D(64, 3, activation='relu', padding='same')(x_reflectance)
x_reflectance = layers.MaxPooling1D(2)(x_reflectance)

# 合并两个通道的特征
x = layers.Concatenate()([x_intensity, x_reflectance])

# 应用注意力层
x = AttentionLayer()(x)

# 展平层，将多维输入一维化
x = layers.Flatten()(x)

# 添加全连接层，128 个神经元，激活函数使用 ReLU
x = layers.Dense(128, activation='relu')(x)

# 添加输出层，输出类别的数量
output_layer = layers.Dense(len(np.unique(y_encoded)), activation='softmax')(x)

# 14. 编译模型
model = models.Model(inputs=[input_intensity, input_reflectance], outputs=output_layer)
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

# 15. 训练模型
model.fit([X_train_reshaped, X_train_reshaped], y_train, epochs=10, batch_size=32, validation_data=([X_test_reshaped, X_test_reshaped], y_test))

# 16. 评估模型
loss, accuracy = model.evaluate([X_test_reshaped, X_test_reshaped], y_test)
print(f"\n双通道 CNN + 注意力机制 测试集准确率: {accuracy}")

# 17. 预测并打印分类报告
y_pred = model.predict([X_test_reshaped, X_test_reshaped])
y_pred_classes = np.argmax(y_pred, axis=1)

print("\n双通道 CNN + 注意力机制 分类结果：")
print(classification_report(y_test, y_pred_classes, target_names=le.classes_))

"""# **打印模型构架**"""

import os
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.metrics import classification_report
import tensorflow as tf
from tensorflow.keras import layers, models

# 数据集路径
INTENSITY_DATA_PATH = '/content/drive/My Drive/Intensity_Dataset'
ACTIVE_LIGHT_DATA_PATH = '/content/drive/My Drive/Reflectance_Dataset'
MERGED_DATA_PATH = '/content/drive/My Drive/merged_dataset.csv'

# 1. 读取合并后的数据
final_merged_data = pd.read_csv(MERGED_DATA_PATH)

# 2. 检查数据基本信息
print("\n合并后的数据集基本信息：")
print(final_merged_data.info())

# 3. 打印前几行，预览数据
print("\n合并后的数据集预览：")
print(final_merged_data.head())

# 4. 数据清洗：删除没有用的数据列（如"label"列）
# 将标签列移到最后，去掉多余的非数值型列
data = final_merged_data.drop(columns=['label'], axis=1)

# 5. 将数据中的非数值类型转换为数值
# 将无法转换的值变成 NaN
data = data.apply(pd.to_numeric, errors='coerce')

# 6. 处理缺失值
# 删除含有 NaN 的行
data_cleaned = data.dropna()

# 7. 确保标签列 'label' 也同步删除对应的行
y_cleaned = final_merged_data['label'][data_cleaned.index]

# 8. 数据检查：检查是否存在非数值数据
if not np.issubdtype(data_cleaned.values.dtype, np.number):
    print("数据集中仍然存在非数值型数据，请检查并清理数据。")
    exit()

# 9. 特征缩放
# 采用标准化，将每列数据转换为均值为0，标准差为1
scaler = StandardScaler()
X_scaled = scaler.fit_transform(data_cleaned)  # 特征缩放

# 10. 标签编码：将标签列 'label' 转换为数值
le = LabelEncoder()
y_encoded = le.fit_transform(y_cleaned)  # 标签编码

# 11. 划分训练集和测试集：80% 训练集，20% 测试集
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded)

# 12. 调整输入数据形状：CNN 要求输入是三维数据
# 1D CNN 的输入形状通常是 (samples, time_steps, features)
X_train_reshaped = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)  # 将输入转化为 (batch_size, time_steps, features)
X_test_reshaped = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)  # 将测试集输入转化为相同的形状

# 自定义注意力层
class AttentionLayer(layers.Layer):
    def __init__(self, **kwargs):
        super(AttentionLayer, self).__init__(**kwargs)

    def build(self, input_shape):
        # 定义注意力权重矩阵
        self.attention_weights = self.add_weight(name='attention_weights',
                                                 shape=(input_shape[-1], 1),
                                                 initializer='random_normal',
                                                 trainable=True)
        super(AttentionLayer, self).build(input_shape)

    def call(self, inputs):
        # 计算注意力权重
        attention_score = tf.matmul(inputs, self.attention_weights)
        attention_score = tf.nn.softmax(attention_score, axis=1)
        # 按照权重加权平均
        weighted_output = inputs * attention_score
        return tf.reduce_sum(weighted_output, axis=1)

# 13. 构建 1D 双通道 CNN 模型

# 添加双输入通道的卷积层
input_intensity = layers.Input(shape=(X_train_reshaped.shape[1], 1))
input_reflectance = layers.Input(shape=(X_train_reshaped.shape[1], 1))

# 第一个通道：Intensity 数据卷积层
x_intensity = layers.Conv1D(32, 3, activation='relu', padding='same')(input_intensity)
x_intensity = layers.MaxPooling1D(2)(x_intensity)
x_intensity = layers.Conv1D(64, 3, activation='relu', padding='same')(x_intensity)
x_intensity = layers.MaxPooling1D(2)(x_intensity)

# 第二个通道：Reflectance 数据卷积层
x_reflectance = layers.Conv1D(32, 3, activation='relu', padding='same')(input_reflectance)
x_reflectance = layers.MaxPooling1D(2)(x_reflectance)
x_reflectance = layers.Conv1D(64, 3, activation='relu', padding='same')(x_reflectance)
x_reflectance = layers.MaxPooling1D(2)(x_reflectance)

# 合并两个通道的特征
x = layers.Concatenate()([x_intensity, x_reflectance])

# 应用注意力层
x = AttentionLayer()(x)

# 展平层，将多维输入一维化
x = layers.Flatten()(x)

# 添加全连接层，128 个神经元，激活函数使用 ReLU
x = layers.Dense(128, activation='relu')(x)

# 添加输出层，输出类别的数量
output_layer = layers.Dense(len(np.unique(y_encoded)), activation='softmax')(x)

# 14. 编译模型
model = models.Model(inputs=[input_intensity, input_reflectance], outputs=output_layer)
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

# 打印模型架构
model.summary()

# 15. 训练模型
model.fit([X_train_reshaped, X_train_reshaped], y_train, epochs=10, batch_size=32, validation_data=([X_test_reshaped, X_test_reshaped], y_test))

# 16. 评估模型
loss, accuracy = model.evaluate([X_test_reshaped, X_test_reshaped], y_test)
print(f"\n双通道 CNN + 注意力机制 测试集准确率: {accuracy}")

# 17. 预测并打印分类报告
y_pred = model.predict([X_test_reshaped, X_test_reshaped])
y_pred_classes = np.argmax(y_pred, axis=1)

print("\n双通道 CNN + 注意力机制 分类结果：")
print(classification_report(y_test, y_pred_classes, target_names=le.classes_))











import os
import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report
from imblearn.over_sampling import SMOTE  # 导入SMOTE库用于过采样

# 数据集路径
REFLECTANCE_DATA_PATH = '/content/drive/My Drive/Reflectance_Dataset'

# 加载数据集并生成标签
def load_dataset_with_labels(directory):
    data_list = []
    labels = []
    for root, dirs, files in os.walk(directory):
        for file in files:
            if file.endswith('.csv'):
                file_path = os.path.join(root, file)
                try:
                    # 尝试多种编码方式读取文件
                    data = None
                    for encoding in ['utf-8-sig', 'latin1', 'gbk', 'utf-8', 'unicode_escape']:
                        try:
                            data = pd.read_csv(file_path, encoding=encoding, on_bad_lines='skip', low_memory=False, header=None)
                            print(f"文件 {file_path} 成功读取 (编码: {encoding})")
                            break  # 如果读取成功，跳出循环
                        except UnicodeDecodeError:
                            print(f"读取文件 {file_path} 时使用编码 {encoding} 失败，尝试其他编码...")

                    if data is None:
                        raise ValueError(f"所有编码方式均无法读取文件 {file_path}")

                    # 转换所有数据为数值类型，无法转换的设为 NaN
                    data = data.apply(pd.to_numeric, errors='coerce')

                    # 删除全空的行和列
                    data = data.dropna(axis=0, how='all')
                    data = data.dropna(axis=1, how='all')

                    # 检查数据是否非空
                    if data.shape[0] > 0 and data.shape[1] > 0:
                        # 使用文件名作为标签
                        label = os.path.splitext(file)[0]
                        data_list.append(data)
                        labels.extend([label] * len(data))
                    else:
                        print(f"文件 {file_path} 的数据在清理后为空，跳过")

                except Exception as e:
                    print(f"读取文件 {file_path} 失败: {e}")
                    continue

    if len(data_list) == 0:
        raise ValueError(f"文件夹 {directory} 中的数据为空或格式不正确")

    return pd.concat(data_list, ignore_index=True), labels

# 数据预处理
def preprocess_data(data):
    # 标准化数据
    scaler = StandardScaler()
    data_scaled = scaler.fit_transform(data)
    return data_scaled

# 定义 ResNet 模型
class BasicBlock(nn.Module):
    def __init__(self, in_channels, out_channels, stride=1):
        super(BasicBlock, self).__init__()
        self.conv1 = nn.Conv1d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1)
        self.bn1 = nn.BatchNorm1d(out_channels)
        self.relu = nn.ReLU(inplace=True)
        self.conv2 = nn.Conv1d(out_channels, out_channels, kernel_size=3, stride=1, padding=1)
        self.bn2 = nn.BatchNorm1d(out_channels)

        self.shortcut = nn.Sequential()
        if stride != 1 or in_channels != out_channels:
            self.shortcut = nn.Sequential(
                nn.Conv1d(in_channels, out_channels, kernel_size=1, stride=stride),
                nn.BatchNorm1d(out_channels)
            )

    def forward(self, x):
        out = self.relu(self.bn1(self.conv1(x)))
        out = self.bn2(self.conv2(out))
        out += self.shortcut(x)
        out = self.relu(out)
        return out

class ResNet1D(nn.Module):
    def __init__(self, input_dim, num_classes):
        super(ResNet1D, self).__init__()
        self.initial_conv = nn.Conv1d(1, 64, kernel_size=7, stride=2, padding=3)
        self.layer1 = self._make_layer(64, 128, stride=2)
        self.layer2 = self._make_layer(128, 256, stride=2)
        self.layer3 = self._make_layer(256, 512, stride=2)
        self.fc = nn.Linear(512, num_classes)

    def _make_layer(self, in_channels, out_channels, stride):
        return nn.Sequential(
            BasicBlock(in_channels, out_channels, stride),
            BasicBlock(out_channels, out_channels)
        )

    def forward(self, x):
        out = self.initial_conv(x)
        out = self.layer1(out)
        out = self.layer2(out)
        out = self.layer3(out)
        out = out.mean(dim=2)  # 全局平均池化
        out = self.fc(out)
        return out

# 主流程
def main():
    try:
        # 加载数据集
        data, labels = load_dataset_with_labels(REFLECTANCE_DATA_PATH)

        # 数据预处理
        X = preprocess_data(data)
        y = np.array(labels)

        # 标签编码
        le = LabelEncoder()
        y = le.fit_transform(y)

        # 使用 SMOTE 对训练数据进行过采样（缓解类别不平衡）
        smote = SMOTE(random_state=42)
        X_resampled, y_resampled = smote.fit_resample(X, y)

        # 划分训练集和测试集
        X_train, X_test, y_train, y_test = train_test_split(
            X_resampled, y_resampled, test_size=0.2, random_state=42, stratify=y_resampled
        )

        # 转换为 PyTorch 张量
        X_train = torch.tensor(X_train[:, np.newaxis, :], dtype=torch.float32)
        X_test = torch.tensor(X_test[:, np.newaxis, :], dtype=torch.float32)
        y_train = torch.tensor(y_train, dtype=torch.long)
        y_test = torch.tensor(y_test, dtype=torch.long)

        # 定义模型
        num_classes = len(le.classes_)
        model = ResNet1D(input_dim=X_train.shape[2], num_classes=num_classes)

        # 初始化 device
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        model.to(device)

        # 计算类别权重，确保每个类别的权重
        class_counts = torch.bincount(y_train)  # 获取每个类的样本数量
        class_weights = 1.0 / class_counts.float()  # 计算类别权重
        class_weights = class_weights / class_weights.sum()  # 归一化类别权重
        class_weights = class_weights.to(device)

        # 定义优化器和损失函数（加权交叉熵损失函数）
        criterion = nn.CrossEntropyLoss(weight=class_weights)
        optimizer = optim.Adam(model.parameters(), lr=0.001)

        # 训练模型
        num_epochs = 50  # 增加训练轮数
        X_train, y_train = X_train.to(device), y_train.to(device)
        X_test, y_test = X_test.to(device), y_test.to(device)

        for epoch in range(num_epochs):
            model.train()
            optimizer.zero_grad()
            outputs = model(X_train)
            loss = criterion(outputs, y_train)
            loss.backward()
            optimizer.step()
            print(f"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}")

        # 测试模型
        model.eval()
        with torch.no_grad():
            outputs = model(X_test)
            _, predicted = torch.max(outputs, 1)
            accuracy = accuracy_score(y_test.cpu(), predicted.cpu())
            print(f"分类准确率: {accuracy * 100:.2f}%")
            print("分类报告:")
            print(classification_report(y_test.cpu(), predicted.cpu(), target_names=le.classes_))

    except Exception as e:
        print(f"出错: {e}")

if __name__ == "__main__":
    main()















"""SVM,KNN,1D ResNet, 1D CNN双通道，分类任务进行对比实验

"""

import os

def check_file_types(directory):
    # 获取文件夹内所有文件
    file_types = {}

    # 遍历文件夹及子文件夹
    for root, dirs, files in os.walk(directory):
        for file in files:
            # 获取文件的扩展名
            file_extension = os.path.splitext(file)[1]

            # 统计每种类型的文件
            if file_extension not in file_types:
                file_types[file_extension] = 1
            else:
                file_types[file_extension] += 1

    return file_types

# 反射率数据集路径
reflectance_dataset_path = '/content/drive/My Drive/Reflectance_Dataset'
# 强度数据集路径
intensity_dataset_path = '/content/drive/My Drive/Intensity_Dataset'

# 检查文件类型
reflectance_file_types = check_file_types(reflectance_dataset_path)
intensity_file_types = check_file_types(intensity_dataset_path)

print(f"Reflectance Dataset File Types: {reflectance_file_types}")
print(f"Intensity Dataset File Types: {intensity_file_types}")

import os
import pandas as pd

# 尝试不同编码读取文件
def read_csv_with_encodings(file_path, encodings=['utf-8', 'gbk', 'latin1']):
    for encoding in encodings:
        try:
            print(f"尝试使用编码 {encoding} 读取文件: {file_path}")
            data = pd.read_csv(file_path, encoding=encoding, on_bad_lines='skip')
            print(f"成功使用编码 {encoding} 读取文件")
            return data
        except Exception as e:
            print(f"使用编码 {encoding} 读取文件失败: {e}")
    raise ValueError(f"无法使用已知编码格式读取文件: {file_path}")

# 加载单个数据集
def load_single_dataset(directory, label_prefix=''):
    data_list = []
    for root, dirs, files in os.walk(directory):
        for file in files:
            if file.endswith('.csv'):
                file_path = os.path.join(root, file)
                try:
                    data = read_csv_with_encodings(file_path)
                    # 提取标签，直接使用文件名作为标签
                    filename = os.path.splitext(file)[0]
                    label = ''.join([c for c in filename if c.isalpha() or c.isdigit()])
                    label = label_prefix + label
                    data['label'] = label
                    data_list.append(data)
                except Exception as e:
                    print(f"读取文件 {file_path} 时出错: {e}")
                    continue
    if not data_list:
        raise FileNotFoundError(f"文件夹 {directory} 中未找到 CSV 文件或所有文件读取失败")
    return data_list

import os
import pandas as pd

# 尝试不同编码读取文件
def read_csv_with_encodings(file_path, encodings=['utf-8', 'gbk', 'latin1']):
    for encoding in encodings:
        try:
            print(f"尝试使用编码 {encoding} 读取文件: {file_path}")
            data = pd.read_csv(file_path, encoding=encoding, on_bad_lines='skip')
            print(f"成功使用编码 {encoding} 读取文件")
            return data
        except Exception as e:
            print(f"使用编码 {encoding} 读取文件失败: {e}")
    raise ValueError(f"无法使用已知编码格式读取文件: {file_path}")

# 加载单个数据集
def load_single_dataset(directory, label_prefix=''):
    data_list = []
    for root, dirs, files in os.walk(directory):
        for file in files:
            if file.endswith('.csv'):
                file_path = os.path.join(root, file)
                try:
                    data = read_csv_with_encodings(file_path)
                    # 提取标签，直接使用文件名作为标签
                    filename = os.path.splitext(file)[0]
                    label = ''.join([c for c in filename if c.isalpha() or c.isdigit()])
                    label = label_prefix + label
                    data['label'] = label
                    data_list.append(data)
                except Exception as e:
                    print(f"读取文件 {file_path} 时出错: {e}")
                    continue
    if not data_list:
        raise FileNotFoundError(f"文件夹 {directory} 中未找到 CSV 文件或所有文件读取失败")
    return data_list

# 加载并预处理数据
def load_and_preprocess():
    # 加载数据集
    intensity_data = load_single_dataset(INTENSITY_DATA_PATH, label_prefix='intensity_')
    active_light_data = load_single_dataset(ACTIVE_LIGHT_DATA_PATH, label_prefix='active_')

    # 确保两个数据集有标签列
    if 'label' not in intensity_data[0].columns or 'label' not in active_light_data[0].columns:
        raise ValueError("目标列 'label' 不存在于数据集中")

    # 数据预处理
    X1, X2, y, label_encoder = preprocess_data(intensity_data[0], active_light_data[0])

    return X1, X2, y, label_encoder















import os
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Conv1D, MaxPooling1D, Flatten, Dense, Dropout, Multiply, GlobalAveragePooling1D, GlobalMaxPooling1D, Add
from tensorflow.keras.callbacks import ReduceLROnPlateau
from tensorflow.keras.utils import to_categorical
from sklearn.metrics import classification_report, confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

# 设置主动光源数据集路径
ACTIVE_LIGHT_DATA_PATH = '/content/drive/My Drive/主动光源数据集'

def load_active_light_data():
    """仅加载主动光源数据集的函数"""
    print("\n主动光源数据集文件夹内的所有文件:")
    active_light_files = []
    for root, dirs, files in os.walk(ACTIVE_LIGHT_DATA_PATH):
        for file in files:
            if file.endswith('.csv'):
                active_light_files.append(os.path.join(root, file))
                print(os.path.join(root, file))

    # 如果没有找到文件
    if not active_light_files:
        raise FileNotFoundError("主动光源数据集文件未找到！")

    # 合并主动光源数据集
    active_light_data = pd.DataFrame()
    encodings = ['gbk', 'gb2312', 'gb18030', 'utf-8']

    try:
        for file_path in active_light_files:
            for encoding in encodings:
                try:
                    temp_data = pd.read_csv(file_path, encoding=encoding, low_memory=False)
                    # 从文件名中提取标签
                    label = os.path.basename(file_path).split('.')[0]
                    temp_data['label'] = label
                    print(f"成功使用 {encoding} 编码读取文件: {file_path}")
                    active_light_data = pd.concat([active_light_data, temp_data], axis=0, ignore_index=True)
                    break
                except UnicodeDecodeError:
                    continue
            else:
                print(f"无法读取文件: {file_path}")

        # 检查数据是否为空
        if active_light_data.empty:
            raise ValueError("加载后的数据为空，请检查文件内容！")

        # 删除空列
        active_light_data.dropna(axis=1, how='all', inplace=True)

        # 检查并删除可能全为空的 `label` 列
        if 'label' not in active_light_data.columns or active_light_data['label'].isnull().all():
            raise ValueError("无法生成有效的标签列，请检查文件名或数据格式！")

        print("\n主动光源数据集加载完成！")
        print(f"合并后的数据形状: {active_light_data.shape}")
        return active_light_data
    except Exception as e:
        print(f"加载主动光源数据时出错: {e}")
        return None

def preprocess_data(data):
    """数据预处理函数"""
    print("\n开始数据预处理...")

    # 删除空值过多的列
    threshold = 0.8  # 超过80%缺失值的列将被删除
    data = data.loc[:, data.isnull().mean() < threshold]
    print(f"清理后剩余列数: {data.shape[1]}")

    # 删除空值行
    data.dropna(inplace=True)
    print(f"清理后数据形状: {data.shape}")

    # 检查是否还存在数据
    if data.empty:
        raise ValueError("数据在清洗后为空，请检查数据质量！")

    # 2. 分离特征和标签
    if 'label' not in data.columns:
        raise ValueError("标签列 'label' 不存在，请检查数据！")

    X = data.drop('label', axis=1).values
    y = data['label'].values

    # 3. 对标签进行编码
    le = LabelEncoder()
    y = le.fit_transform(y)
    y = to_categorical(y)  # 转换为独热编码
    print(f"标签映射关系: {dict(zip(le.classes_, range(len(le.classes_))))}")

    # 4. 划分训练集和测试集
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y.argmax(axis=1)
    )

    # 5. 特征标准化
    scaler = StandardScaler()
    X_train = scaler.fit_transform(X_train)
    X_test = scaler.transform(X_test)

    # 调整形状以适应 1D CNN
    X_train = X_train[..., np.newaxis]
    X_test = X_test[..., np.newaxis]

    print("数据预处理完成！")
    print(f"训练集形状: {X_train.shape}")
    print(f"测试集形状: {X_test.shape}")

    return X_train, X_test, y_train, y_test, le.classes_

def attention_block(inputs):
    """简单的自注意力机制"""
    avg_pool = GlobalAveragePooling1D()(inputs)
    max_pool = GlobalMaxPooling1D()(inputs)

    dense_avg = Dense(inputs.shape[-1], activation='relu')(avg_pool)
    dense_max = Dense(inputs.shape[-1], activation='relu')(max_pool)

    attention = Add()([dense_avg, dense_max])
    attention = Dense(inputs.shape[-1], activation='sigmoid')(attention)
    output = Multiply()([inputs, attention])
    return output

def build_cnn_with_attention(input_shape, num_classes):
    """构建带注意力机制的1D CNN模型"""
    inputs = Input(shape=input_shape)
    x = Conv1D(64, kernel_size=3, activation='relu', padding='same')(inputs)
    x = MaxPooling1D(pool_size=2)(x)
    x = Dropout(0.3)(x)

    # 注意力机制
    x = attention_block(x)

    x = Conv1D(128, kernel_size=3, activation='relu', padding='same')(x)
    x = MaxPooling1D(pool_size=2)(x)
    x = Dropout(0.3)(x)

    x = Flatten()(x)
    x = Dense(128, activation='relu')(x)
    x = Dropout(0.5)(x)
    outputs = Dense(num_classes, activation='softmax')(x)

    model = Model(inputs, outputs)
    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
    return model

def train_and_evaluate_cnn(X_train, X_test, y_train, y_test, class_names):
    """训练和评估1D CNN模型"""
    input_shape = X_train.shape[1:]
    num_classes = len(class_names)

    # 构建模型
    model = build_cnn_with_attention(input_shape, num_classes)
    model.summary()

    # 学习率调度器
    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-6, verbose=1)

    # 训练模型
    history = model.fit(
        X_train, y_train,
        validation_data=(X_test, y_test),
        epochs=20,
        batch_size=32,
        verbose=1,
        callbacks=[reduce_lr]
    )

    # 评估模型
    y_pred = model.predict(X_test).argmax(axis=1)
    y_true = y_test.argmax(axis=1)

    print("\n分类报告:")
    print(classification_report(y_true, y_pred, target_names=class_names))

    # 混淆矩阵
    conf_matrix = confusion_matrix(y_true, y_pred)
    plt.figure(figsize=(12, 8))
    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)
    plt.title('混淆矩阵')
    plt.xlabel('预测标签')
    plt.ylabel('真实标签')
    plt.tight_layout()
    plt.show()

    return model, history

def main():
    # 1. 加载数据
    active_light_data = load_active_light_data()
    if active_light_data is None:
        return

    # 2. 数据预处理
    X_train, X_test, y_train, y_test, class_names = preprocess_data(active_light_data)

    # 3. 训练和评估改进的1D CNN
    model, history = train_and_evaluate_cnn(X_train, X_test, y_train, y_test, class_names)
    print("\n分类任务完成！")

if __name__ == "__main__":
    main()





